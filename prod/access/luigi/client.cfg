[core]
scheduler_host=access
parallel_scheduling=True
log_level=INFO
logging_conf_file=/etc/luigi/logging.cfg

[state]
folder=/var/task-state

[worker]
task_limit=10000

[resources]
hdfs=50
qa-wayback=10
w3act=6

[scheduler]
batch_emails = True
record_task_history = True
state_path = /var/tmp/luigid-state.pickle

[task_history]
db_connection = sqlite:////var/tmp/luigid-task-hist.db

[hadoop]
command=/usr/bin/hadoop
streaming-jar=/usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2-cdh3u1.jar
version=cdh3
python-executable=/usr/local/bin/python2.7

[hdfs]
client=webhdfs
#namenode_host=192.168.45.60 # ingest VM
# crawler02
namenode_host=192.168.45.19
namenode_port=50070
effective_user=access
tmp_dir=/tmp

[webhdfs]
port=14000
user=access

[email]
receiver=andrew.jackson@bl.uk

[batch_notifier]
batch_mode=family

[systems]
elasticsearch_host=access
elasticsearch_port=9200
elasticsearch_index_prefix=access-tasks

[act]
url=http://crawler03.bl.uk:9000/act
username=w3act-access@bl.uk
password=password
